
@article{baker2016,
  title = {1,500 Scientists Lift the Lid on Reproducibility},
  volume = {533},
  abstract = {Survey sheds light on the `crisis' rocking research.},
  language = {en},
  number = {7604},
  journal = {Nature News},
  doi = {10.1038/533452a},
  author = {Baker, Monya},
  month = may,
  year = {2016},
  pages = {452},
  file = {/Users/swindecker/Zotero/storage/FQ8WZ9VK/1-500-scientists-lift-the-lid-on-reproducibility-1.html}
}

@incollection{fidler2018,
  edition = {Winter 2018},
  title = {Reproducibility of {{Scientific Results}}},
  abstract = {The terms ``reproducibility crisis'' and ``replicationcrisis'' gained currency in conversation and in print over thelast decade (e.g., Pashler \& Wagenmakers 2012), as disappointingresults emerged from large scale reproducibility projects in variousmedical, life and behavioural sciences (e.g., Open ScienceCollaboration, OSC 2015). In 2016, a poll conducted by the journalNature reported that more than half (52\%) of scientistssurveyed believed science was facing a ``replicationcrisis'' (Baker 2016). More recently, some authors have moved tomore positive terms for describing this episode in science; forexample, Vazire (2018) refers instead to a ``credibilityrevolution'' highlighting the improved methods and open sciencepractices it has motivated., The crisis often refers collectively to at least the following things:, The associated open science reform movement aims to rectify conditionsthat led to the crisis. This is done by promoting activities such asdata sharing and public pre-registration of studies, and by advocatingstricter editorial policies around statistical reporting includingpublishing replication studies and statistically non-significantresults., This review consists of four distinct parts. First, we look at theterm ``reproducibility'' and related terms like``repeatability'' and ``replication'', presentingsome definitions and conceptual discussion about the epistemicfunction of different types of replication studies. Second, wedescribe the meta-science research that has established andcharacterised the reproducibility crisis, including large scalereplication projects and surveys of questionable research practices invarious scientific communities. Third, we look at attempts to addressepistemological questions about the limitations of replication, andwhat value it holds for scientific inquiry and the accumulation ofknowledge. The fourth and final part describes some of the manyinitiatives the open science reform movement has proposed (and in manycases implemented) to improve reproducibility in science. In addition,we reflect there on the values and norms which those reforms embody,noting their relevance to the debate about the role of values in thephilosophy of science.},
  booktitle = {The {{Stanford Encyclopedia}} of {{Philosophy}}},
  publisher = {{Metaphysics Research Lab, Stanford University}},
  author = {Fidler, Fiona and Wilcox, John},
  editor = {Zalta, Edward N.},
  year = {2018},
  file = {/Users/swindecker/Zotero/storage/ACJGKV33/scientific-reproducibility.html}
}

@article{lowndes2017,
  title = {Our Path to Better Science in Less Time Using Open Data Science Tools},
  volume = {1},
  copyright = {2017 Nature Publishing Group},
  issn = {2397-334X},
  abstract = {Reproducibility has long been a tenet of science but has been challenging to achieve\textemdash{}we learned this the hard way when our old approaches proved inadequate to efficiently reproduce our own work. Here we describe how several free software tools have fundamentally upgraded our approach to collaborative research, making our entire workflow more transparent and streamlined. By describing specific tools and how we incrementally began using them for the Ocean Health Index project, we hope to encourage others in the scientific community to do the same\textemdash{}so we can all produce better science in less time.},
  language = {en},
  number = {6},
  journal = {Nature Ecology \& Evolution},
  doi = {10.1038/s41559-017-0160},
  author = {Lowndes, Julia S. Stewart and Best, Benjamin D. and Scarborough, Courtney and Afflerbach, Jamie C. and Frazier, Melanie R. and O'Hara, Casey C. and Jiang, Ning and Halpern, Benjamin S.},
  month = jun,
  year = {2017},
  pages = {0160},
  file = {/Users/swindecker/Zotero/storage/HWJD2J2Q/Lowndes et al. - 2017 - Our path to better science in less time using open.pdf;/Users/swindecker/Zotero/storage/B5U2MD7D/s41559-017-0160.html}
}

@article{stevens2018,
  title = {Building a Local Community of Practice in Scientific Programming for {{Life Scientists}}},
  abstract = {In this paper, we describe why and how to build a local community of practice in scientific programming for life scientists that use computers and programming in their research. A community of practice is a small group of scientists that meet regularly to help each other and promote good practices in scientific programming. While most life scientists are well-trained in the laboratory to conduct experiments, good practices with (big) datasets and their analysis are often missing. We propose a model on how to build such a community of practice at a local academic institution, present two real-life examples and introduce challenges and implemented solutions. We believe that the current data deluge that life scientists face can benefit from the implementation of these small communities. Good practices spread among experimental scientists will foster open, transparent and sound scientific results beneficial to society.},
  language = {en},
  journal = {bioRxiv},
  doi = {10.1101/265421},
  author = {Stevens, Sarah L. R. and Kuzak, Mateusz and Martinez, Carlos and Moser, Aurelia and Bleeker, Petra and Galland, Marc},
  month = nov,
  year = {2018},
  file = {/Users/swindecker/Zotero/storage/GQTJBN6K/Stevens et al. - 2018 - Building a local community of practice in scientif.pdf}
}

@article{buck2015,
  title = {Solving Reproducibility},
  volume = {348},
  copyright = {Copyright \textcopyright{} 2015, American Association for the Advancement of Science},
  issn = {0036-8075, 1095-9203},
  abstract = {![Figure][1]{$<$}/img{$>$}

PHOTO: LAURA AND JOHN ARNOLD FOUNDATION

The reproducibility problem in science is a familiar issue, not only within the scientific community, but with the general public as well. Recent developments in social psychology (such as fraudulent research by D. Stapel) and cell biology (the Amgen Inc. and Bayer AG reports on how rarely they could reproduce published results) have become widely known. Nearly every field is affected, from clinical trials and neuroimaging, to economics and computer science. Obvious solutions include more research on statistical and behavioral fixes for irreproducibility, activism for policy changes, and demanding more pre-registration and data sharing from grantees. Two Perspectives in this issue (pp. [1420][2] and [1422][3]) describe how journals and academic institutions can foster a culture of reproducibility. Transparency is central to improving reproducibility, but it is expensive and time-consuming. What can be done to alleviate those obstacles?

![Figure][1]{$<$}/img{$>$}

``Transparency is central to improving reproducibility\ldots{}'' 

PHOTO: \textcopyright{} EDWARD ROZZO/CORBIS

Most scientists aspire to greater transparency, but if being transparent taps into scarce grant money and requires extra work, it is unlikely that scientists will be able to live up to their own cherished values. Thus, one of the most effective ways to promote high-quality science is to create free open-source tools that give scientists easier and cheaper ways to incorporate transparency into their daily workflow: from open lab notebooks, to software that tracks every version of a data set, to dynamic document generation. Moreover, scientists who use open-source software are not locked into proprietary software platforms with unclear monetization plans. If philanthropy or government funds new tools that the open-source community can iterate and improve on, the per-dollar return on investment can far exceed the costs.

Infrastructural tools are now available, or in development, that should help to catalyze a change in scientific transparency. One example is the Open Science Framework (OSF), a free and open-source software platform for managing scientific workflow (supported by the Laura and John Arnold Foundation in partnership with the Center for Open Science). Among its many features, this platform can enable scientists to easily track the history of all versions of every document or data set and the exact contributions made by each team member. All project materials can be given persistent identifiers, and the tracking of provenance allows any subsequent research project to give proper credit to the original. Projects using this platform include the Shared Access Research Ecosystem project of the Association of Research Libraries and its partners. This project endeavors to connect scholarly metadata and allow the identification of various elements of a research project, such as grant proposals, journal articles, and data repository information.

Open-source platform innovations are growing. Other examples include the iPython project (supported largely by the Alfred P. Sloan Foundation), which offers a web-based computing notebook for users to create documents such as code, computational results, and narrative explanations. Although originally developed around the Python language, the project has expanded to cover other languages (such as R) under the banner of Project Jupyter. The Galaxy Project (funded by the U.S. National Science Foundation and others) provides a web-based platform for ``data-intensive'' biomedical research; and for managing bioinformatics and phylogenetic research on plants, there is the iPlant Collaborative.

The scientific community cannot depend entirely on volunteers or the private market to develop free platforms that address specialized scientific needs and encourage greater reproducibility. Ultimately, the infrastructure supporting science is a public good, just like the knowledge it produces. By supporting such infrastructure, the public and philanthropic sectors can make it painless for researchers to live up to their own values of openness and rigor.

 [1]: pending:yes
 [2]: /lookup/doi/10.1126/science.aab3847
 [3]: /lookup/doi/10.1126/science.aab2374},
  language = {en},
  number = {6242},
  journal = {Science},
  doi = {10.1126/science.aac8041},
  author = {Buck, Stuart},
  month = jun,
  year = {2015},
  pages = {1403-1403},
  file = {/Users/swindecker/Zotero/storage/A3PGA3AH/Buck - 2015 - Solving reproducibility.pdf;/Users/swindecker/Zotero/storage/3YEDT3QX/1403.html},
  pmid = {26113692}
}

@article{fidler2017,
  title = {Metaresearch for {{Evaluating Reproducibility}} in {{Ecology}} and {{Evolution}}},
  volume = {67},
  issn = {0006-3568},
  abstract = {Abstract.  Recent replication projects in other disciplines have uncovered disturbingly low levels of reproducibility, suggesting that those research literature},
  language = {en},
  number = {3},
  journal = {BioScience},
  doi = {10.1093/biosci/biw159},
  author = {Fidler, Fiona and Chee, Yung En and Wintle, Bonnie C. and Burgman, Mark A. and McCarthy, Michael A. and Gordon, Ascelin},
  month = mar,
  year = {2017},
  pages = {282-289},
  file = {/Users/swindecker/Zotero/storage/CW5VW6XF/Fidler et al. - 2017 - Metaresearch for Evaluating Reproducibility in Eco.pdf;/Users/swindecker/Zotero/storage/ULNF3XHL/2900173.html}
}

@techreport{marwick2018,
  title = {Packaging Data Analytical Work Reproducibly Using {{R}} (and Friends)},
  abstract = {Computers are a central tool in the research process, enabling complex and large scale data analysis. As computer-based research has increased in complexity, so have the challenges of ensuring that this research is reproducible. To address this challenge, we review the concept of the research compendium as a solution for providing a standard and easily recognisable way for organising the digital materials of a research project to enable other researchers to inspect, reproduce, and extend the research. We investigate how the structure and tooling of software packages of the R programming language are being used to produce research compendia in a variety of disciplines. We also describe how software engineering tools and services are being used by researchers to streamline working with research compendia. Using real-world examples, we show how researchers can improve the reproducibility of their work using research compendia based on R packages and related tools.},
  language = {en},
  number = {e3192v2},
  institution = {{PeerJ Inc.}},
  author = {Marwick, Ben and Boettiger, Carl and Mullen, Lincoln},
  month = mar,
  year = {2018},
  file = {/Users/swindecker/Zotero/storage/H2AN4JL7/Marwick et al. - 2018 - Packaging data analytical work reproducibly using .pdf;/Users/swindecker/Zotero/storage/XK7GN4W9/3192.html},
  doi = {10.7287/peerj.preprints.3192v2}
}


